{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "816a4c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6aeefdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "GRID_SIZE = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19fcbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_iteration(GAMMA=0.9, non_endpoint_reward=-1):\n",
    "    # Grid parameters\n",
    "    ACTIONS = {'UP': 0, 'DOWN': 1, 'LEFT': 2, 'RIGHT': 3}\n",
    "    TERMINAL_STATES = [0, GRID_SIZE*GRID_SIZE-1]\n",
    "    THETA = 1e-4\n",
    "\n",
    "    def get_next_state(state, action):\n",
    "        row, col = state // GRID_SIZE, state % GRID_SIZE # split into rows & cols -- easier to understand\n",
    "        if action == ACTIONS['UP']: row = max(0, row - 1)\n",
    "        elif action == ACTIONS['DOWN']: row = min(GRID_SIZE - 1, row + 1)\n",
    "        elif action == ACTIONS['LEFT']: col = max(0, col - 1)\n",
    "        elif action == ACTIONS['RIGHT']: col = min(GRID_SIZE - 1, col + 1)\n",
    "        return row * GRID_SIZE + col # merge back\n",
    "    \n",
    "    # Reward model: Received upon ENTERING next_state\n",
    "    # Terminal states: R(s') = +10, non-terminal: R(s') = 0\n",
    "    # V(terminal) = 0 by definition (no future rewards)\n",
    "    def get_reward(next_state):\n",
    "        if next_state == 0: return 10\n",
    "        elif next_state == GRID_SIZE*GRID_SIZE-1: return 10\n",
    "        else: return non_endpoint_reward\n",
    "        \n",
    "    # Initialize value function\n",
    "    V = np.zeros(GRID_SIZE * GRID_SIZE)\n",
    "\n",
    "    # Value Iteration\n",
    "    while True:\n",
    "        delta = 0\n",
    "        new_V = V.copy()\n",
    "        for s in range(len(V)):\n",
    "            if s in TERMINAL_STATES:\n",
    "                continue  # Skip terminal states\n",
    "            max_val = -float('inf')\n",
    "            for action in ACTIONS.values():\n",
    "                s_next = get_next_state(s, action)\n",
    "                reward = get_reward(s_next)\n",
    "                val = reward + GAMMA * V[s_next]\n",
    "                if val > max_val:\n",
    "                    max_val = val\n",
    "            new_V[s] = max_val\n",
    "            delta = max(delta, abs(new_V[s] - V[s]))\n",
    "        V = new_V\n",
    "        # Display results\n",
    "        print(\"Value Function (4x4 Grid):\")\n",
    "        print(V.reshape(GRID_SIZE, GRID_SIZE))\n",
    "        if delta < THETA:\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Extract optimal policy\n",
    "    policy = [None] * (GRID_SIZE * GRID_SIZE)\n",
    "    for s in range(len(V)):\n",
    "        if s in TERMINAL_STATES:\n",
    "            continue\n",
    "        best_action = None\n",
    "        best_value = -float('inf')\n",
    "        for action_name, action_val in ACTIONS.items():\n",
    "            s_next = get_next_state(s, action_val)\n",
    "            reward = get_reward(s_next)\n",
    "            val = reward + GAMMA * V[s_next]\n",
    "            if val > best_value: # obtain the max\n",
    "                best_value = val\n",
    "                best_action = action_name\n",
    "        policy[s] = best_action\n",
    "\n",
    "    return V, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67ed3551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Function (4x4 Grid):\n",
      "[[ 0. 10.  0.  0.]\n",
      " [10.  0.  0.  0.]\n",
      " [ 0.  0.  0. 10.]\n",
      " [ 0.  0. 10.  0.]]\n",
      "Value Function (4x4 Grid):\n",
      "[[ 0. 10.  5.  0.]\n",
      " [10.  5.  0.  5.]\n",
      " [ 5.  0.  5. 10.]\n",
      " [ 0.  5. 10.  0.]]\n",
      "Value Function (4x4 Grid):\n",
      "[[ 0.  10.   5.   2.5]\n",
      " [10.   5.   2.5  5. ]\n",
      " [ 5.   2.5  5.  10. ]\n",
      " [ 2.5  5.  10.   0. ]]\n",
      "Value Function (4x4 Grid):\n",
      "[[ 0.  10.   5.   2.5]\n",
      " [10.   5.   2.5  5. ]\n",
      " [ 5.   2.5  5.  10. ]\n",
      " [ 2.5  5.  10.   0. ]]\n",
      "\n",
      "Optimal Policy (Directions):\n",
      "[[None 'LEFT' 'LEFT' 'DOWN']\n",
      " ['UP' 'UP' 'UP' 'DOWN']\n",
      " ['UP' 'UP' 'DOWN' 'DOWN']\n",
      " ['UP' 'RIGHT' 'RIGHT' None]]\n"
     ]
    }
   ],
   "source": [
    "V1, policy1 = val_iteration(0.5, 0)\n",
    "print(\"\\nOptimal Policy (Directions):\")\n",
    "print(np.array(policy1).reshape(GRID_SIZE, GRID_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1f0298",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258bcacc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
